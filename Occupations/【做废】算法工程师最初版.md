## 人工智能和相关技术对中国就业的影响

经济学家预测，到2030年，人工智能将为全球经济带来15.7万亿美元的财富

人工智能及相关技术在未来20年将取代中国现有的26%的工作岗位，高于对英国的20%的预估，但也能通过提升工作效率和实际收入在中国创造大量新工作机会。根据PWC中央估计值，人工智能对中国就业的净影响可能将创造约12%的竞增岗位，相当于未来20年内增加9,000个就业岗位。

中国大部分新增岗位预计将出现在服务业，预计净增长29%(约9,700万)，尤其是医疗保健等子行业，可能会出现大幅度增长，我们预计建筑业的净增长幅度将达到23%（1,400万），人工智能对工业领域的就业岗位净影响大致偏中性。而预计农业的净流失岗位约为2,200万

**替代效应**人工智能及其相关科技取代人工的潜力
![1.png](https://i.loli.net/2019/04/12/5cafd6d2b4a05.png)
![2.png](https://i.loli.net/2019/04/12/5cafd6d0f1dab.png)
![3.png](https://i.loli.net/2019/04/12/5cafd6d309b0d.png)
![4.png](https://i.loli.net/2019/04/12/5cafd6d2aa95b.png)
![5.png](https://i.loli.net/2019/04/12/5cafd6d353570.png)
![6.png](https://i.loli.net/2019/04/12/5cafd6d39fd75.png)
![7.png](https://i.loli.net/2019/04/12/5cafd6d0c640a.png)
![8.png](https://i.loli.net/2019/04/12/5cafd6d13f298.png)
![9.png](https://i.loli.net/2019/04/12/5cafd74543b44.png)
![10.png](https://i.loli.net/2019/04/12/5cafd74497888.png)
![11.png](https://i.loli.net/2019/04/12/5cafd744959ba.png)
![12.png](https://i.loli.net/2019/04/12/5cafd744e824b.png)
![13.png](https://i.loli.net/2019/04/12/5cafd7462297a.png)
![14.png](https://i.loli.net/2019/04/12/5cafd744be784.png)

## 一、岗位

### 新增岗位

- AI撰稿人，撰写AI客户服务chatbots使用的副本;
- AI团队的律师，负责管理宝贵的AI知识产权和法律问题;
- 技术销售总监;将AI创新带入现场，将这些服务与潜在客户联系起来;
- AI分析师和战略顾问，为使用和构建AI技术的雇主提供咨询和战略建议;
- AI团队的营销经理，为将AI技术作为产品或服务的公司建立知名度和高级客户群;
- 用户体验或AI的“UX”设计人员，他们是创造性人才，负责为客户构建优雅易用的AI界面;
- AI新闻工作者，覆盖快速发展的深度学习和人工智能行业的新闻。 

* 无人驾驶汽车调度员
* AI立法专家、学者
* AI训练师
* 机器人任务编排专家
* 方言采集者
* 

## 二、职责

### 主要职责

1. 与我们的数据科学家和软件架构师紧密合作，开发和维护我们的核心产品
2. 评估和分析现有的数据，构造新的数据集，以产生基本的见解，或用于培训 NLP 模型
3. 开发高度可伸缩的代码，利用自然语言处理和基于规则的模型集成到我们的数据管道中
4. 调整、部署、评估和改进 NLP 模型(Python、 Tensorflow、 Jupyter 笔记本)
5. 致力于新特性的实现，保证正确的实现，良好的文档化和按时交付
6. 时刻关注创造和改进的机会。 从我们的开发过程，到最终的用户体验



### 知识、技能和能力

1. 使用 NLP 的经验
2. 使用 Python 的扎实技能
3. 使用最先进的深度学习方法的扎实经验，例如建立神经网络，包括 DNNs，RNNs，bLSTMs，gru 和 / 或在该领域的出版记录
4. 优秀的分析、概念和解决问题的能力
5. 对敏捷和精益原则的痴迷(GitHub，Trello)

## 三、学习工具

### Dataset release 

1. A paper accepted at EMNLP 2018 presented a benchmark dataset for machine
   translation of noisy text (MTNT), consisting of Reddit comments and
   professionally sourced translations. It differs from previous datasets which are
   mostly synthetically generated
2. Quora has publicly released a dataset called Question Sincerity, where the goal is
   to feed the data to a machine learning model and train it to be able to
   distinguish well-intentioned questions from provocative ones. The idea is to be
   able to capture those ill-intentioned questions, flag them, and remove them from
   the platform so as to reduce any harm that such information could cause to the
   community. The dataset was also released on Kaggle as a competition.
3. Google releases an image captioning dataset called Conceptual Captions. This
   dataset was released as part of a research paper presented at ACL 2018
4. Gab.ai corpus is a large-scale dataset for studying hate speech and toxicity on
   social media platforms.
5. A new large-scale dataset and task for visual common sense reasoning has been
   made publicly available, with the goal to enable cognition-level understanding in
   AI systems.
6. Facebook and New York University released a dataset called XNLI which was
   created for evaluating cross-lingual approaches to natural language
   understanding (NLU). XNLI also includes baselines that can assist researchers to
   create systems that understand multiple languages.
7. ShaRC is a dataset that focuses on building end to end conversational question
   answering systems that support the addition of background knowledge,
   especially when the system tries to answer a more difficult question, where the
   answer is not directly in the text source.
8. Question Answering in Context (QuAC), is a dataset consisting of data instances
   representing dialog between two crowd workers. It is useful for modeling,
   understanding, and participating in information seeking dialog.
9. Amazon releases sales rank dataset for kindle and print books.
10. Dataset released by Rada Mihalcea can be used to build fake news detection
    systems.
11. Alexa scientist releases FEVER, a public dataset containing 185,000 data instances
    useful for fact extraction and verification.
12. IBM releases a dataset which includes information that can be leveraged to build
    more comprehensive QA systems based on knowledge and reasoning.
13. In an effort to build more representative ML models and promote inclusiveness in
    AI, Google AI announces the Inclusive Images Competition on Kaggle. The
    challenge is to build robust image captioning tools that work even for images
    that contain underrepresented groups based on the Open Images dataset.
14. TwentyBN has released a massive video dataset (Something-Something V2) to
    help enable systems to have the ability of video understanding and visual
    common sense.
15. A new version of FastText includes pre-trained word embeddings in 157
    languages, a resource which is useful for those working on multilingual research.
16. Microsoft releases a massive collection of free datasets for advancing NLP and
    machine learning research.
17. SWAG is a new natural language inference dataset which was presented at the
    EMNLP 2018 conference. The dataset consists of adversarial generated and
    human-verified question-answer pairs, which are useful to test language
    modeling, question-answering, or natural language inference systems.
18. DeepMind open sources their dataset used to train generative query networks
    (GQNs) for neural scene representation and rendering.
19. A song lyric toxicity dataset is made available accompanied by analysis and slides.
20. FAIR releases dataset which can be useful to train AI Agents to teach other visual
    navigation.
21. Here is a nice dataset which contains short jokes scraped from various websites.
22. A dataset and model for comprehending paragraphs describing processes.
23. Google releases a dataset and challenge for landmark recognition.
24. DataTurks offers several open datasets and offers a neat interface to explore
    them.
25. CoDraw is a dataset used for training a task that enables collaborative drawing
    between two agents using natural language understanding from dialog.
26. Microsoft released 125 million building footprints from the US as open data
27. IBM releases a dataset of recorded debates containing 60 speeches (audio + ASR
    & human transcripts). Useful for conducting different NLP tasks such as argument
    detection and argument stance classification. 
28. Here is an alphabetical list of NLP related datasets.
29. Google announces Open Image V4 which contains 15.4 bounding-boxes for 600
    categories on 1.9M images. Google claims this is the largest existing dataset with
    object location annotations. 
30. Amazon is working on making a large-scale fact extraction and verification
    dataset publicly available. Learn more about their efforts here. 

### learning resources

1. ## Deep Learning and Reinforcement Learning Summer School, Toronto 2018

2. Elvis Saravia and Soujanya Poria release a project called NLP-Overview that is
   intended to help students and practitioners get a condensed overview of modern
   deep learning techniques applied to NLP, including theory, algorithms,
   applications, and state-of-the-art result.

3. Lecture material is available for Sebastian Raschka’s new ML course given at the
   University of Washington Madison.

4. OpenAI releases a new educational package, called Spinning Up in Deep RL, for
   those interested in learning about the topic of deep reinforcement learning. It
   includes an extensive list of algorithms and resources used to effectively train
   deep reinforcement learning algorithms.

5.  Yandex School of Data Analysis (YSDA) releases material for their new NLP course
   (GitHub repo).

6. NYU announces a course called “Neural Aesthetics” for learning how to teach
   different artistic capabilities to neural networks.

7. Stanford released a new course called “TensorFlow for Deep Learning Research”.
   It contains full lecture notes and slides covering topics such as convnets,
   generative adversarial networks (GANs), transformer, Tensor2Tensor, and much
   more. 

8. Stanford also released slides for all the NLP related seminars for Fall 2018, which
   include emerging topics such as multi-task learning, semantic role labeling, and
   visual questions answering.

9. Deeplizard releases a new course covering the fundamentals of neural networks
   and tensor math, taught purely with PyTorch.

10. Shervine Amidi releases a neat website containing several deep learning and
    machine learning cheat-sheets. The guides are available in several languages. 

11. Berkley AI Research (BAIR) offers new “Intro to AI” course

12. Rule of Machine Learning:Best Practices for ML engineering- for deploying realworld ML-based apps provided by Google’s ML team.

13. Here is a mini-course on Deep Learning with PyTorch (lecture slides and code
    included).

14. Bloomberg is offering a new course on “Foundations of Machine Learning”

15. Facebook is investing efforts to teach machine learning in an intuitive way (in a
    six-part video series) to learners from all different backgrounds.

16. # The University of Texas is offering a new course on Linear Algebra hosted on edX.

17. Sebastian Ruder releases NLP Progress, a repository to keep track of state-of-theart results in NLP research.

18. Google releases new machine learning course for free.

19. As part of the ML Education project, Google releases ML Learning Guides which
    are a set of tutorials to teach step-by-step machine learning essentials and best
    practices

20. Moustapha Cisse, the founder of Google AI Ghana, introduces new one-year
    intensive Master’s Program for Machine Intelligence in Africa.

21. The Gradient is a publication that aims to democratize AI through educational
    content.

22. The Gradient is a publication that aims to democratize AI through educational
    content.

23. Washington University (in St. Louis) opens course “T81-558: Applications of Deep
    Neural Networks” (includes Jupyter notebooks).

24. Yann LeCun, Mikael Henaff, and Alfredo Canziani released a new course on deep
    learning that aims to teach the latest techniques in deep learning and
    representation learning. 

25. Stanford released a new course for NLP called “CS224n: NLP with Deep Learning”.

26. Egor Polusmak releases a machine learning course which consists of topics such
    as visual data analysis with Python and unsupervised learning.

27. Deeplearning.ai releases new course on sequence models, introducing topics
    such as GRUs, LSTMs, and Recurrent Neural Networks (RNNs). This specialization
    is taught by Andrew Ng. 



### BOOKS

1. Deeplearning.ai releases new course on sequence models, introducing topics
   such as GRUs, LSTMs, and Recurrent Neural Networks (RNNs). This specialization
   is taught by Andrew Ng. 
2. Will Ratcliff discuss methods to capture the curiosity and imagination of your
   audience when delivering scientific presentations.
3. Andrew Trask releases his new book entitled “Grokking Deep Learning” where he
   aims to teach deep learning and related mathematical concepts in a more
   intuitive way using Numpy (notebooks included).
4. Terence Parr and Jeremy Howard released an online book called “The Matrix
   Calculus You Need for Deep Learning”, which discusses the fundamental math
   you need for applying deep learning to various problems.
5. “Fairness and Machine Learning: Limitations and Opportunities” is an online
   textbook discussing fairness in machine learning.
6. Andrew Ng releases a book called “Machine Learning Yearning” where he teaches
   how to structure Machine Learning projects.
7. Goku Mohandas released a set of notebooks (called Practical AI) that teaches
   how to program machine learning models using a practical approach via PyTorch.
8. Bharath Ramsundar and Reza Zadeh published a book on “TensorFlow for Deep
   Learning” which covers topics that range from linear regression to Reinforcement
   Learning.
9. Ian Goodfellow publicly releases LaTeX files for the notations used in his “Deep
   Learning” book. 

### Newsletters and Podcasts

1. This week in AI by Denny Britz 
2. NLP News by Sebastian Ruder 
3. NLP Highlights (podcast) by Matt Gardner and Waleed Ammar 
4. Deep Learning Weekly 
5. nathan.ai newsletter by Nathan Benaich 
6. Alignment Newsletter 
7. AI Diary by Elvis Saravia 
8. The Creative AI Newsletter 
9. NLP Newsletter by Elvis Saravia 

### Open source

t, the field wouldn’t be where it is now if it were not
for the great efforts made by developers as well. It’s important to understand the role software
engineering plays in research.

1. Google AI releases Dopamine, a TensorFlow-based framework that provides
   flexibility, stability, and reproducibility for new and experienced reinforcement
   learning researchers.
2. TensorFlow announces Tensorflow.js, a Web-GL accelerated, browser-based
   JavaScript library for training and deploying ML models on the web.
3. TensorFlow releases a beginner’s guide on how to apply probabilistic
   programming to real-world problems. It basically serves as a guide on how to
   leverage TensorFlow Probability, a library built for scientists, statisticians, and ML
   researchers, to encode domain knowledge to understand data and make
   predictions.
4. Google releases Cirq, an open source tool which enables researchers to write
   quantum algorithms for quantum processors such as those used for NISQ
   computers.
5. GluonNLP is a tool for reproducing state-of-the-art research in NLP. It provides
   several implementations and features that makes it easier to build NLP-based
   prototypes and products.
6. The Facebook AI team has released pytext, a set of NLP libraries and pre-built
   models built on top of PyTorch that provide functionalities to build NLP 
   46 applications that scale and are efficient at inference time. The library is mostly
   aimed at developers rather than researchers.
7. Google AI releases What-If Tool, a TensorBoard feature to help users better
   understand their machine learning models without writing code.
8. The TensorFlow team releases AutoAugment which consists of a series of
   modules provided on TFHub that allows researchers to train better image models
   with fewer data using image augmentation tricks.
9. Google releases Dataset Search, a platform to quickly and efficiently search and
   find open datasets which have been uploaded to public sites such as personal
   websites and university profiles.
10. Earlier last week, Facebook AI research team released wav2letter — “a simple and
    efficient end-to-end Automatic Speech Recognition (ASR) system.” The toolkit
    provides pre-trained models that can get you started right away with transcribing
    speech. The accompanying paper can also be found here.
11. The Facebook research team introduces DensePose, a real-time approach for
    mapping human pixels from 2D images to 3D surface model of a human body.
12. The Natural Language Decathlon (decaNLP) benchmark offers a unique setting
    for studying general NLP models that can perform several natural language tasks.
13. DGL is a library to build graph neural networks including Graph Convolutional
    Networks.
14. With TensorBoardX, now you can use TensorBoard with many other deep
    learning frameworks such as PyTorch, Chainer, MXNet, among others
15. Google's MLPerf is an effort that aims to build a common set of machine learning
    benchmarks to measure system performance for both training and inference
    from mobile devices to cloud services.
16. Salesforce’s Einstein AI team releases TransmogrifAI, an AutoML library that
    focuses on accelerating machine learning developer productivity through
    automated machine learning for structured data.
17. Papers with Code is a web tool for searching machine learning papers that
    contain open source code. 
18. The MXNet team releases GluonCV, a deep learning toolkit that lets engineers
    and researchers quickly develop new algorithms and baselines (for image
    recognition, object detection, and semantic segmentation)
19. DeepSuperLearner is an implementation of deep ensemble methods for tackling
    classification problems.
20. Pythia is a modular framework that won the visual question answering challenge
    presented by FAIR
21. OpenAI announced an efficient method, called gradient-checkpointing, to train
    memory-efficient and fast deep neural networks
22. You can find all of Facebook’s open source projects here. It includes PyTorch,
    Caffe2, ONNX, Tensor Comprehensions and much more. 
23. Google offers free access to GPUs on their online collaborative notebook service
    called Google Colab. 
24. ml5.js is an open source web development tool, built on top of Tensorflow.js, that
    lets users easily access machine learning algorithms and models on the browser.
25. FAIR releases code for the paper "Colorless green recurrent networks dream
    hierarchically”. This is another work that aims to understand what recurrent
    architectures are learning and to what extent they are effective at modeling
    hierarchical structure
26. FAIR releases PyTorch implementation of “Poincaré Embeddings for Learning
    Hierarchical Representations”. This paper was published in NeurIPS 2017 and
    proposed an approach for learning hierarchical representations of symbolic data
    by embedding then into hyperbolic space.
27. JupyterLab is released in beta version, and developers say that it is ready for mass
    user adoption. 
28. The Facebook AI research team releases a more efficient implementation of RCNN and Mask R-CNN using PyTorch 1.0. The modular implementations can be
    used for instance segmentation and object detection.
29. Peter Norvig releases Lisp code for the textbook “Paradigms of Artificial
    Intelligence Programming”.
30. TensorFlow implementation of the Capsule Network model implemented in the
    paper “Dynamic Routing between Capsules”. 
31. TextQL is a library that allows you to execute against CSV or TSV text files.
32. PolygonRNN++ is a new interactive annotation tool for segmentation datasets. 
33. FAIR share open source tools for fastMRI with the aim to increase the efficiency
    and speed of processing MRI scans (up to 10x times faster).
34. CoinRun is a training environment developed by OpenAI which provides a metric
    for an agent’s ability to generalize across new environments, which is a
    challenging task in deep reinforcement learning.
35. DeepMind releases GraphNets, which is an easy-to-use library for training graphbased neural networks
36. Stateoftheart.ai is a platform for crowdsourcing state-of-the-art results in
    different areas of AI research such as reinforcement learning and natural
    language processing
37. Muppet Show is a web interface that allows you to explore how language is seen
    and analyzed by models such as BERT and ELMo. It consists of a visualization tool
    where you can query for specific tokens and find similar contexts to them.
38. Jonathan Kummerfeld has developed an impressive web page to visualize and
    showcase a neural Part-of-Speech tagger using various deep learning toolkits
    such as Dynet, PyTorch, and TensorFlow.
39. Zaid Alyafeai releases code for his image-to-image translation system which
    operates on the browser. The system implements the pix2pix paper and was
    developed to work on the browser using Tensorflow.js.
40. Here is a beautiful website called REGEXPER which allows you to test your regular
    expressions on the browser. 
41. A new medical imaging framework (Medical Torch), based on PyTorch, has been
    open sourced by Christian Perone. The idea with this tool is to simplify the steps
    of pre-processing MRI data.
42. Code release for MojiTalk, which is a system used to generate emotional
    responses at scale.
43. TensorFlow introduces SPINN, a tool that enables natural language
    understanding in TensorFlow with eager execution.
44. Sublime Text 3.1 is released. 
45. Test Tube is a library to track and optimize deep learning experiments which
    allows researchers to easily log experiments and parallelize hyperparameter
    search.
46. Facebook AI Research (FAIR) release "Tensor Comprehensions", which is a library
    that aims to bridge the gap between researchers and engineers who work
    together in ML research and implementations.
47. TorchFold is a tool built on top of PyTorch that makes it easy to batch anything
    regardless of the complexity of your dynamic architectures
48. NCRF++ is an open-source neural sequence labeling toolkit.
49. HuggingFace introduces NeuralCoref which is a fast and efficient coreference
    resolution tool built with neural networks and SpaCy.
50. TextDistance is a python library for comparing the distance between two or more
    sequences. Implements various algorithms such as Hamming, Jaccard index, etc.
51. DL-Text is a repository containing code on how to pre-process textual data for
    deep learning models built with TensorFlow. 
